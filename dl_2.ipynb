{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VarshithaNuligonda/DL-assignment2/blob/main/dl_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZGLJks_8HD4",
        "outputId": "58f0d31e-b6cb-482e-a78e-90874af48caf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 4.594659805297852\n",
            "Epoch 2, Loss: 4.535257816314697\n",
            "Epoch 3, Loss: 4.499439239501953\n",
            "Epoch 4, Loss: 4.4116597175598145\n",
            "Epoch 5, Loss: 4.326439380645752\n",
            "Epoch 6, Loss: 4.166933536529541\n",
            "Epoch 7, Loss: 4.1221747398376465\n",
            "Epoch 8, Loss: 3.9705941677093506\n",
            "Epoch 9, Loss: 3.7967562675476074\n",
            "Epoch 10, Loss: 3.703268527984619\n"
          ]
        }
      ],
      "source": [
        "#question-1\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the Seq2Seq model\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, input_vocab_size, target_vocab_size, embedding_dim, hidden_dim, num_layers, cell_type='LSTM'):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "\n",
        "        # Define the embedding layer for input (Latin characters)\n",
        "        self.input_embedding = nn.Embedding(input_vocab_size, embedding_dim)\n",
        "\n",
        "        # Choose RNN type: RNN, LSTM, or GRU\n",
        "        if cell_type == 'LSTM':\n",
        "            self.encoder_rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers)\n",
        "            self.decoder_rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers)\n",
        "        elif cell_type == 'GRU':\n",
        "            self.encoder_rnn = nn.GRU(embedding_dim, hidden_dim, num_layers)\n",
        "            self.decoder_rnn = nn.GRU(embedding_dim, hidden_dim, num_layers)\n",
        "        else:\n",
        "            self.encoder_rnn = nn.RNN(embedding_dim, hidden_dim, num_layers)\n",
        "            self.decoder_rnn = nn.RNN(embedding_dim, hidden_dim, num_layers)\n",
        "\n",
        "        # Output layer for decoder (to generate Devanagari characters)\n",
        "        self.output_layer = nn.Linear(hidden_dim, target_vocab_size)\n",
        "\n",
        "        # Define the dropout for regularization\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        # Input embedding\n",
        "        embedded_src = self.input_embedding(src)\n",
        "\n",
        "        # Encoder pass\n",
        "        _, (hidden, cell) = self.encoder_rnn(embedded_src)\n",
        "\n",
        "        # Prepare for the decoder\n",
        "        trg_len = trg.size(0)\n",
        "        batch_size = trg.size(1)\n",
        "        output = torch.zeros(trg_len, batch_size, self.output_layer.out_features).to(trg.device)\n",
        "\n",
        "        # First input to the decoder is the <sos> token (start of sequence)\n",
        "        input_dec = trg[0, :]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            embedded_trg = self.input_embedding(input_dec).unsqueeze(0)\n",
        "\n",
        "            # Decoder pass\n",
        "            if isinstance(self.decoder_rnn, nn.LSTM):\n",
        "                decoder_out, (hidden, cell) = self.decoder_rnn(embedded_trg, (hidden, cell))\n",
        "            else:\n",
        "                decoder_out, hidden = self.decoder_rnn(embedded_trg, hidden)\n",
        "\n",
        "            # Output layer (to get predictions)\n",
        "            output[t] = self.output_layer(decoder_out.squeeze(0))\n",
        "\n",
        "            # Decide whether to use teacher forcing or not\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = output[t].argmax(1)  # Get the index of the highest probability\n",
        "\n",
        "            # Use the actual next token as the next input if teacher forcing, else use the predicted token\n",
        "            input_dec = trg[t] if teacher_force else top1\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# Parameters for the model\n",
        "input_vocab_size = 100  # Vocabulary size of Latin\n",
        "target_vocab_size = 100  # Vocabulary size of Devanagari\n",
        "embedding_dim = 64  # Embedding dimension\n",
        "hidden_dim = 128  # Hidden state size (k)\n",
        "num_layers = 1  # One layer for encoder and decoder\n",
        "cell_type = 'LSTM'  # Cell type (LSTM, GRU, or RNN)\n",
        "\n",
        "# Instantiate the model\n",
        "model = Seq2Seq(input_vocab_size, target_vocab_size, embedding_dim, hidden_dim, num_layers, cell_type)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Example input and target sequences (Latin and Devanagari characters)\n",
        "# Here the sequences are represented as indices in the vocab\n",
        "# For example, a Latin sequence: \"hello\" = [0, 1, 2, 3, 4], Devanagari target: \"à¤¨à¤®à¤¸à¥à¤¤à¥‡\" = [5, 6, 7, 8, 9]\n",
        "src_example = torch.tensor([[0, 1, 2, 3, 4]]).T  # Example input\n",
        "trg_example = torch.tensor([[5, 6, 7, 8, 9]]).T  # Example target\n",
        "\n",
        "# Training loop (simplified)\n",
        "for epoch in range(10):  # Iterate for 10 epochs as an example\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(src_example, trg_example)\n",
        "    output_dim = output.shape[-1]\n",
        "\n",
        "    output = output[1:].view(-1, output_dim)\n",
        "    trg = trg_example[1:].view(-1)\n",
        "\n",
        "    loss = loss_function(output, trg)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question-2\n",
        "# =======================================\n",
        "# STEP 1: Install Required Libraries\n",
        "# =======================================\n",
        "!pip install transformers datasets --quiet\n",
        "\n",
        "# =======================================\n",
        "# STEP 2: Import Libraries\n",
        "# =======================================\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    GPT2Tokenizer, GPT2LMHeadModel,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer, TrainingArguments\n",
        ")\n",
        "\n",
        "# Disable external logging (like wandb)\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# =======================================\n",
        "# STEP 3: Load Tokenizer and Base GPT-2\n",
        "# =======================================\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # set pad token\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# =======================================\n",
        "# STEP 4: Create Sample Training Data\n",
        "# =======================================\n",
        "lyrics_file = \"lyrics.txt\"\n",
        "if not os.path.exists(lyrics_file):\n",
        "    sample_lyrics = [\n",
        "        \"You're the one that I want\\n\",\n",
        "        \"Hello from the other side\\n\",\n",
        "        \"Cause baby you're a firework\\n\",\n",
        "        \"Let it go, let it go\\n\",\n",
        "        \"We will, we will rock you\\n\"\n",
        "    ]\n",
        "    with open(lyrics_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(sample_lyrics)\n",
        "\n",
        "# =======================================\n",
        "# STEP 5: Load & Tokenize Lyrics Data\n",
        "# =======================================\n",
        "dataset = load_dataset(\"text\", data_files={\"train\": lyrics_file})\n",
        "\n",
        "def tokenize_text(example):\n",
        "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "tokenized_data = dataset.map(tokenize_text, batched=True)\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# =======================================\n",
        "# STEP 6: Set Up Training Arguments\n",
        "# =======================================\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-lyrics-output\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=10,\n",
        "    logging_steps=5,\n",
        "    save_total_limit=1,\n",
        "    prediction_loss_only=True\n",
        ")\n",
        "\n",
        "# =======================================\n",
        "# STEP 7: Train the GPT-2 Model\n",
        "# =======================================\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    data_collator=collator\n",
        ")\n",
        "\n",
        "print(\"ðŸŽ¶ Training GPT-2 on sample lyrics...\")\n",
        "trainer.train()\n",
        "print(\"âœ… Training complete.\")\n",
        "\n",
        "# Save the fine-tuned model and tokenizer for later reuse\n",
        "model.save_pretrained(\"gpt2-lyrics-model\")\n",
        "tokenizer.save_pretrained(\"gpt2-lyrics-model\")\n",
        "\n",
        "# =======================================\n",
        "# STEP 8: Define a Function to Generate Lyrics\n",
        "# =======================================\n",
        "def generate_lyrics(prompt, max_new_tokens=60):\n",
        "    # Encode the user-provided prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    # Generate new tokens based on the prompt\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.7,\n",
        "        top_k=40,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    # Decode and return the generated text\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# =======================================\n",
        "# STEP 9: Get User Song Prompt and Generate Lyrics\n",
        "# =======================================\n",
        "user_prompt = input(\"ðŸŽ¤ Enter your song prompt: \")\n",
        "lyrics = generate_lyrics(user_prompt)\n",
        "print(\"\\nðŸŽµ Generated Lyrics:\")\n",
        "print(lyrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "epuWkCHIKF1M",
        "outputId": "63eaa8e9-ac50-4c5d-c16e-34ed0fce1740"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ¶ Training GPT-2 on sample lyrics...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9/9 01:08, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.968300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Training complete.\n",
            "ðŸŽ¤ Enter your song prompt: baby you light up my world\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽµ Generated Lyrics:\n",
            "baby you light up my world, I need to be you!\"\n",
            "\n",
            "\"I've got to be you!\"\n",
            "\n",
            "\"I've got to be you!\"\n",
            "\n",
            "\"I'm the one you want to be, I'm the one you want to be, I'm the one you want to be, I'm the\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgjFv5NZjUPsR5v0EG1MP+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}